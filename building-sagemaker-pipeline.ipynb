{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530c3d7d",
   "metadata": {},
   "source": [
    "# Adapting Preprocessing Script for SageMaker\n",
    "\n",
    "To build a pipeline, we need to start with individual steps. Our first step will be data preprocessing — the same preprocessing you've done locally in previous courses, but now adapted to run in SageMaker's managed environment.\n",
    "\n",
    "We'll create a separate file called `data_processing.py` that contains our preprocessing logic. The preprocessing logic itself is identical to what you've done before — we're still capping outliers, creating new features, and splitting the data. The key changes are in how we handle file paths to work within SageMaker's processing environment:\n",
    "\n",
    "These specific paths (`/opt/ml/processing/`) are SageMaker conventions that allow the service to automatically handle data movement between S3 and your processing containers. When SageMaker runs this script, it will automatically mount your S3 data to the input directory and upload the results from the output directories back to S3.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f2fabd",
   "metadata": {},
   "source": [
    "# Understanding SageMaker Sessions\n",
    "\n",
    "Before building your pipeline, you need to understand that SageMaker Pipelines require two different types of sessions that serve distinct purposes:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a930736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "# Create a SageMaker session for immediate operations\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Create a pipeline session for pipeline components\n",
    "pipeline_session = PipelineSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c44b418",
   "metadata": {},
   "source": [
    "The distinction between these sessions is about execution context, not timing:\n",
    "\n",
    "- `sagemaker.Session()` — This is your direct connection to AWS services. When you use this session, you're telling SageMaker \"execute this operation using real AWS resources right now.\" It handles immediate operations like uploading data to S3, creating pipeline definitions, starting executions, and checking status.\n",
    "\n",
    "- `PipelineSession()` — This is a special \"recording\" session that creates placeholder operations instead of real ones. When you use this session with processors or estimators, instead of immediately creating SageMaker jobs, it returns step objects that become part of your pipeline definition. These placeholders get converted into real operations only when the pipeline executes.\n",
    "\n",
    "Without `PipelineSession()`, if you created a processor with a regular session, SageMaker would immediately try to spin up compute instances and start processing your data before you've even finished defining your pipeline! The PipelineSession() lets you define all your pipeline components as a complete workflow first, then execute everything in the proper order when you're ready.\n",
    "\n",
    "Simple Rule:\n",
    "\n",
    "- Use `PipelineSession()` for any processor, estimator, or transformer that should become a pipeline step\n",
    "- Use sagemaker.Session() for immediate actions like managing the pipeline itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd28dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default SageMaker bucket name\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# Local file path\n",
    "DATA_PATH = \"data/california_housing.csv\"\n",
    "\n",
    "# S3 prefix (folder path within the bucket)\n",
    "DATA_PREFIX = \"datasets\"\n",
    "\n",
    "try:\n",
    "    # Upload the dataset using the upload_data() method\n",
    "    s3_uri = sagemaker_session.upload_data(\n",
    "        path=DATA_PATH,\n",
    "        bucket=default_bucket,\n",
    "        key_prefix=DATA_PREFIX\n",
    "    )\n",
    "    \n",
    "    print(f\"Data uploaded successfully to: {s3_uri}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cb765a",
   "metadata": {},
   "source": [
    "# Setting Up AWS Resources\n",
    "\n",
    "Now that we understand sessions, we need to set up the AWS resources and permissions that our pipeline will use:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da5ea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve your AWS account ID (used for constructing resource ARNs)\n",
    "account_id = sagemaker_session.boto_session.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "# Get the default S3 bucket for your SageMaker resources\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# Define the SageMaker execution role ARN\n",
    "SAGEMAKER_ROLE = f\"arn:aws:iam::{account_id}:role/SageMakerDefaultExecution\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89608146",
   "metadata": {},
   "source": [
    "# Creating the Processing Environment\n",
    "\n",
    "To run our preprocessing script in SageMaker, we need to define the computing environment where our code will execute. We use an `SKLearnProcessor` because our preprocessing script uses scikit-learn libraries, and crucially, we use the `PipelineSession` to ensure the processor becomes a pipeline step rather than executing immediately:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b0889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "# Create a processor that will run our data preprocessing script\n",
    "processor = SKLearnProcessor(\n",
    "    framework_version=\"1.2-1\",    # Specify scikit-learn version\n",
    "    role=SAGEMAKER_ROLE,          # IAM role with necessary permissions\n",
    "    instance_type=\"ml.m5.large\",  # Compute instance type for processing\n",
    "    instance_count=1,             # Number of instances to use\n",
    "    sagemaker_session=pipeline_session  # Use pipeline session for deferred execution\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293f49fb",
   "metadata": {},
   "source": [
    "# Building Our First Pipeline Step\n",
    "\n",
    "With our processor configured, we can now create the actual processing step using `ProcessingStep`. This step defines what data goes in, what comes out, and what code runs in between.\n",
    "\n",
    "Before proceeding, note that we assume you have already uploaded your raw dataset (`california_housing.csv`) to your S3 default bucket at the path `/datasets/california_housing.csv`. This is necessary because the pipeline will read the input data directly from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6648c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "# Define the processing step with inputs, outputs, and the script to run\n",
    "processing_step = ProcessingStep(\n",
    "    name=\"ProcessData\",   # Unique name for this step in the pipeline\n",
    "    processor=processor,  # The processor we defined above\n",
    "    inputs=[\n",
    "        # Define where the raw data comes from (S3 location)\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            source=f\"s3://{default_bucket}/datasets/california_housing.csv\",\n",
    "            destination=\"/opt/ml/processing/input\"  # Where data will be mounted in container\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        # Define where processed training data will be saved\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"train_data\",               # Reference name for this output\n",
    "            source=\"/opt/ml/processing/train\"       # Container path where script saves data\n",
    "        ),\n",
    "        # Define where processed test data will be saved\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"test_data\",                # Reference name for this output\n",
    "            source=\"/opt/ml/processing/test\"        # Container path where script saves data\n",
    "        )\n",
    "    ],\n",
    "    code=\"data_processing.py\"  # The Python script that performs the processing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a46b0b5",
   "metadata": {},
   "source": [
    "Notice how the paths in our `ProcessingInput` and `ProcessingOutput` definitions perfectly match the paths our script expects. The `inputs` parameter specifies where our raw data comes from (an S3 location) and where it will be mounted inside the processing container (`/opt/ml/processing/input`). The `outputs` parameter defines where our processed data will be saved, with separate outputs for training and test data. Each output has a name that we can reference later and a source path where our processing script will write the data. The `code` parameter points to the Python script that contains our preprocessing logic.\n",
    "\n",
    "This connection between the step definition and the script paths is crucial — it's what allows SageMaker to automatically handle all the data movement for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b4664b",
   "metadata": {},
   "source": [
    "# Creating the Pipeline\n",
    "\n",
    "With our processing step defined, we can now create our first pipeline. A pipeline is simply a collection of steps that execute in order, and right now we have just one step. Note that we use the regular `sagemaker_session` for pipeline management:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd75da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "# Set a name for the SageMaker Pipeline\n",
    "PIPELINE_NAME = \"california-housing-preprocessing-pipeline\"\n",
    "\n",
    "# Create pipeline with our processing step\n",
    "pipeline = Pipeline(\n",
    "    name=PIPELINE_NAME,\n",
    "    steps=[processing_step],  # For now, just one step\n",
    "    sagemaker_session=sagemaker_session  # Use regular session for pipeline management\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2501d054",
   "metadata": {},
   "source": [
    "This creates a pipeline definition with our single processing step. In future lessons, we'll add more steps to this list to create more complex workflows with training, evaluation, and model registration.\n",
    "\n",
    "At this point, we've only created the pipeline definition in memory — it doesn't exist in AWS yet. To make it available in SageMaker, we need to register it with the service using the `upsert` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ca50dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or update pipeline (upsert = update if exists, create if not)\n",
    "pipeline.upsert(role_arn=SAGEMAKER_ROLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30573404",
   "metadata": {},
   "source": [
    "The `upsert` method is particularly useful because it handles both creation and updates intelligently. If this is the first time you're running this code, it will create a new pipeline in SageMaker. If you run the same code again after making changes to your pipeline definition, it will update the existing pipeline rather than throwing an error. This makes iterative development much smoother — you can modify your pipeline code, run it again, and SageMaker will automatically apply your changes.\n",
    "\n",
    "Think of the pipeline definition as a blueprint or recipe. Once you've registered this blueprint with SageMaker using `upsert`, you can execute it multiple times. Each execution is a separate run of the same blueprint, potentially with different data or parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029ded7e",
   "metadata": {},
   "source": [
    "# Executing the Pipeline\n",
    "\n",
    "Now that our pipeline is registered with SageMaker, we can start an execution. This is where the actual work begins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db90ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start pipeline execution and get execution object for monitoring\n",
    "execution = pipeline.start()\n",
    "\n",
    "# Get the execution ARN for tracking\n",
    "print(f\"Pipeline execution ARN: {execution.arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b742f5",
   "metadata": {},
   "source": [
    "When you call `pipeline.start()`, SageMaker immediately begins executing your pipeline in the background. This means your local Python script doesn't need to wait for the processing to complete — the heavy computational work is happening on AWS infrastructure while your script continues running or even after it finishes.\n",
    "\n",
    "The execution object provides valuable information about the running pipeline. You'll see output similar to:\n",
    "\n",
    "`\n",
    "Pipeline execution ARN: arn:aws:sagemaker:us-east-1:123456789012:pipeline/california-housing-preprocessing-pipeline/execution/x1gc33lgj8v5\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e4c50b",
   "metadata": {},
   "source": [
    "# Monitoring Your Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de9f54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the current status\n",
    "execution_details = execution.describe()\n",
    "\n",
    "# Display status\n",
    "print(f\"Status: {execution_details['PipelineExecutionStatus']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
